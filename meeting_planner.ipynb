{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting planner\n",
    "This notebook is controlled by the SICB Program Officer to manage the development of an annual meeting.\n",
    "This can be run on Google CoLab, or the PO's personal machine, provided it is configured to run Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System configuration\n",
    "Execute the following cell on the first occasion to install the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following libraries are needed for preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# The following is needed to run GPT-4\n",
    "# ! pip install openai # This version works on my local machine\n",
    "! pip install openai==0.28 # This version works with Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary items\n",
    "\n",
    "You'll have to execute this cell each time you work with the notebook because it defines the paths and imports the essential packages.\n",
    "The first time you run it, you will want to adjust the paths for data_root and code_root for your system. \n",
    "Also, make sure that the abstracts downloaded from X-CD are stored in the data_root directory in the data_root path. \n",
    "The root_path should also have [keywords.xlsx](), which details the initial keywords to be used for the GPT ratings.\n",
    "A template for [keywords.xlsx]() should be included in the conference_planner repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:09:14.232682Z",
     "start_time": "2023-12-09T23:09:13.294533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import outside packages\n",
    "import os, sys\n",
    "\n",
    "# Mount Google Drive, if running on Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set the data root to a Google Drive folder\n",
    "    data_root = '/content/drive/MyDrive/meeting_planning_2024'\n",
    "    code_root = '/content/drive/MyDrive/Colab Notebooks/conference_planner'\n",
    "\n",
    "    # Add code to path\n",
    "    sys.path.append(code_root)\n",
    "\n",
    "# If running locally, set the data root\n",
    "else:\n",
    "    data_root = '/Users/mmchenry/Documents/Projects/meeting_planner_test'\n",
    "\n",
    "# Import conference_planner code\n",
    "import make_sessions as ms\n",
    "import preprocessing as pp\n",
    "\n",
    "# Abstract data file, without its extension, saved at data_root (an xlsx file, downloaded from X-CD)\n",
    "abstract_filename = 'abstracts_123852'\n",
    "\n",
    "# Check the directory structure and create if necessary\n",
    "pp.setup_directories(data_root, abstract_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the abstracts\n",
    "This section is intended to flag and filter out duplicate and otherwise problematic abstract submissions.\n",
    "Only needs to be run once, when the abstracts come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds columns to the abstracts data, renames some columns, and saves the result as abstracts_revised.xlsx\n",
    "pp.process_abstracts(data_root, abstract_filename)\n",
    "\n",
    "# Flag abstracts to exclude, due to duplicate titles or IDs. Save abstracts_revised.xlsx with the column 'exclude' added. Also save abstracts_excluded.xlsx with the excluded abstracts.\n",
    "pp.flag_duplicates(data_root)\n",
    "\n",
    "# Identify authors that submitted multiple abstracts. Save list to 'duplicate_primary_contacts.xlsx'\n",
    "pp.find_duplicate_authors(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create divisional files\n",
    "This distributes the abstract and keyword files to the divisional directories.\n",
    "You will want to be sure to set the edit permission of each directory to enable the corresponding DPOs to edit these files. \n",
    "The DPOs can then edit the keywords, if they like, before GPT's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:26:58.232901Z",
     "start_time": "2023-12-09T23:24:38.059300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load each type of abstract, save each type of presentation in separate csv files\n",
    "pp.distribute_abstracts(data_root)\n",
    "\n",
    "# Creates a csv file for recording the keywords ratings used by GPT\n",
    "pp.setup_ratings(data_root)\n",
    "\n",
    "# Creates XLSX files for each division to adjust the weights for each keyword\n",
    "pp.setup_weights(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running GPT-4\n",
    "\n",
    "Once the DPOs have approved of their keywords, the cell below uses GPT-4 to rate how well each keyword characterizes each abstract.\n",
    "The ratings are provided on a scale from 0 to 1.\n",
    "Note that this step costs money, so you will ideally run this only once.\n",
    "\n",
    "This cell does use the [OpenAI API](https://openai.com/blog/openai-api) and so requires an account. \n",
    "The account number should stored in a text file that should keep the number protected, as anyone who has it could charge queries to the OpenAI servers, which incurs charges.\n",
    "\n",
    "Note that if the code fails (e.g., the OpenAI account runs out of money or the servers are down), then you can simply restart it and it will pick up from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_work as gpt\n",
    "\n",
    "# Path to text file for the OPEN-AI API key\n",
    "path_to_API_key = '/Users/mmchenry/Documents/code/openai_api_key.txt'\n",
    "\n",
    "# Run GPT to generate keyword ratings\n",
    "gpt.analyze_abstracts(data_root, path_to_API_key, max_attempts=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After GPT has generated keyword ratings for each abstract, the DPOs can use [meeting_planner](meeting_planner.ipynb) to adjust their sessions.\n",
    "It would be a good idea to download a copy of all folders and files as a backup. \n",
    "If any of the GPT ratings files are accidentally deleted, then you will want to be able to restore them without having to re-run GPT-4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
